{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from utilities import get_nodeid2text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the text data + labels\n",
    "nodeid2text = pd.read_pickle(\"data/nodeid2text_gensim.pkl\")\n",
    "# load splits\n",
    "_, (train_idx, valid_idx, test_idx) = get_nodeid2text()\n",
    "# load the output of the LDA model\n",
    "num_topics_list = [10, 20, 40, 80]\n",
    "all_gammas = [\n",
    "    torch.from_numpy(np.load(f\"gammas/{n_topics}_topics.npy\")) for n_topics in tqdm(num_topics_list)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = list()\n",
    "valid_ds = list()\n",
    "for i in range(len(num_topics_list)):\n",
    "    train_ds.append(TensorDataset(all_gammas[i][train_idx], torch.from_numpy(nodeid2text.loc[train_idx][\"label\"].values)))\n",
    "    valid_ds.append(TensorDataset(all_gammas[i][valid_idx], torch.from_numpy(nodeid2text.loc[valid_idx][\"label\"].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, input_size, n_hidden):\n",
    "        super(DenseNet, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, 40),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        return logits\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    return correct, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = list()\n",
    "valid_dl = list()\n",
    "valid_acc = list()\n",
    "for train_set, valid_set in zip(train_ds, valid_ds):\n",
    "    train_dl.append(DataLoader(train_set, batch_size=64, pin_memory=True))\n",
    "    valid_dl.append(DataLoader(valid_set, batch_size=64, pin_memory=True))\n",
    "for i, n_topics in enumerate(num_topics_list):\n",
    "    print(f\"Num Topics: {n_topics} -----\")\n",
    "    model = DenseNet(input_size=n_topics, n_hidden=80).to(device)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    epochs = 10\n",
    "    for t in range(epochs):\n",
    "        train_loop(train_dl[i], model, loss_fn, optimizer)\n",
    "        correct, test_loss = test_loop(valid_dl[i], model, loss_fn)\n",
    "        print(f\"Epoch {t+1}: Valid Acc: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\")\n",
    "    valid_acc.append(correct)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from LDA_LogisticRegression.ipynb notebook\n",
    "lda_lr_valid_acc = [0.444478, 0.44142421, 0.53783684, 0.56149535,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"LDA Validation Acc. v. Num Topics\")\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "xaxis = np.arange(len(num_topics_list))\n",
    "plt.bar(xaxis-0.2, valid_acc, width=0.4, label=\"LDA+DenseNet\")\n",
    "plt.bar(xaxis+0.2, lda_lr_valid_acc, width=0.4, label=\"LDA+LR\")\n",
    "plt.xticks(xaxis, [str(n_topics) for n_topics in num_topics_list])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "010d058ca0eabdc86ef3c4af3f31bd9cb145f230d71eb139e3a74b00188222a9"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('cs267a')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
